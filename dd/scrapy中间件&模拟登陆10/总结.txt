1. middlewares.py中间件的使用
	# 根据逻辑位置不同分为
		# 爬虫中间件
		# 下载中间件
	# 预处理request、response对象
		# 替换User-Agent
		# 替换Cookies
		# 使用代理ip
		# 检查响应
	# 需要在settings.py中设置开启
		# 左位置，右权重
		# 权重值越小，越优先执行
	# 中间件中的函数
		process_request(request, spider)
			# 当request经过中间件时被调用
			# 不能写return，让request正常的继续流转
		process_response(response, request, spider)
			# 当response经过中间件时被调用
			# 正常return response，让response正常的继续流转
			# 参数request是经过的response对应的请求对象
		# return request-->引擎-->调度器-->入队
		# return response--。。。-->爬虫-->解析

2. crawlspider爬虫类
	class CrawlSpider(scrapy.spiders.CrawlSpider):
		name = 爬虫名
		allowed_domains = []
		start_urls = []
		# 不能写名为parse的函数，该函数用来实现rules规则元祖的部分功能
		# 多了一个rules规则元祖
		rules = (
			Rule(LinkExtractor(规则参数))
				# 链接提取器按照规则参数提取url，构造request；框架会发送请求，获取响应

			Rule(LinkExtractor(规则参数),
				 follow=True)
				# 链接提取器按照规则参数提取url，构造request；框架会发送请求，获取响应
				# 该响应(链接提取器按照规则参数提取url对应的响应)继续会进入rules规则元祖中，被提取处理

			Rule(LinkExtractor(规则参数),
				 callback='解析函数名的字符串')
				# 链接提取器按照规则参数提取url，构造request；框架会发送请求，获取响应
				# 该响应(链接提取器按照规则参数提取url对应的响应)会进入callback指定的回调解析函数中进行提取

			Rule(LinkExtractor(规则参数),
				 callback='解析函数名的字符串',
				 follow=True) # 规则对象
				# 链接提取器按照规则参数提取url，构造request；框架会发送请求，获取响应
				# 该响应(链接提取器按照规则参数提取url对应的响应)会进入callback指定的回调解析函数中进行提取
				# 该响应(链接提取器按照规则参数提取url对应的响应)也继续会进入rules规则元祖中，被提取处理

			# LinkExtractor链接提取器的参数
				# allow：按照正则匹配a标签中href属性的值
				# restrict_xpaths：xpath规则定位标签，该标签范围内的url都会被提取
				# 其他参数：deny allow_domains deny_domains
		)

	# Rule规则对象中callback指定的回调解析函数之间不能传递数据
	# 除此以外，写法用法原理和scrapy.Spider爬虫类完全一致

3. scrapy_redis作用
	断点续爬
	实现分布式

4. scrapy_redis能够实现断点续爬以及分布式的原理
	在共用的redis中持久化了request队列和指纹集合
	并且，获取的数据都保存在共用的那个redis中

5. scrapy_redis工作流程
	在scrapy工作流程的基础上，用共用的redis存放request队列和指纹集合，并保存数据