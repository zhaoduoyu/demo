1. scrapy的setting.py配置
	a. 常用配置
		ROBOTSTXT_OBAY = False
		USER_AGENT
		DEFAULT_REQUEST_HEADERS # 里边写ua没用

		ITEM_PIPELINES
		SPIDER_MIDDLEWARES
		DOWNLOADER_MIDDLEWARES
			# 左位置右权重，值越小先执行

		COOKIES_DEBUG = False 不显示cookies的传递日志信息
		COOKIES_ENABLE = True 开启cookies的传递

		LOG_LEVEL = 'DEBUG' 控制日志输出的等级
		LOG_FILE 指定日志文件，终端将不再显示日志信息，受LOG_LEVEL的控制

		DOWNLOAD_DELAY = 0 下载延迟的时间 单位 秒
		CONCURRENT_REQUESTS = 16 同时最大发送请求的数量

	b. scrapy_redis的配置
		DUPEFILTER_CLASS 指纹去重类
		SCHEDULER 调度器类
		SCHEDULER_PERSIST = True 持久化请求队列和指纹集合
		ITEM_PIPELINES = {'scrapy_redis.pipelines.RedisPipeline':400}
		REDIS_URL

	c. scrapy_splash的配置
		DUPEFILTER_CLASS 指纹去重类
		DOWNLOADER_MIDDLEWARES = {三个scrapy_splash的中间件}
		HTTPCACHE_STORAGE 使用splash的http缓存
		SPLASH_URL

	d. scrapy_redis和scrapy_splash配合使用的配置

		SCHEDULER 调度器类
		SCHEDULER_PERSIST = True 持久化请求队列和指纹集合
		ITEM_PIPELINES = {'scrapy_redis.pipelines.RedisPipeline':400}
		REDIS_URL

		DOWNLOADER_MIDDLEWARES = {三个scrapy_splash的中间件}
		HTTPCACHE_STORAGE 使用splash的http缓存
		SPLASH_URL

		DUPEFILTER_CLASS = 指定自定义重写的指纹去重类

	e. 重写的指纹去重类
		from copy import deepcopy
		from scrapy.utils.request import request_fingerprint
		from scrapy.utils.url import canonicalize_url
		from scrapy_splash.utils import dict_hash
		from scrapy_redis.dupefilter import RFPDupeFilter
		def splash_request_fingerprint(request, include_headers=None):
		    fp = request_fingerprint(request, include_headers=include_headers)
		    if 'splash' not in request.meta: return fp
		    splash_options = deepcopy(request.meta['splash'])
		    args = splash_options.setdefault('args', {})
		    if 'url' in args:
		        args['url'] = canonicalize_url(args['url'], keep_fragments=True)
		    return dict_hash(splash_options, fp)
		class SplashAwareDupeFilter(RFPDupeFilter):
		    def request_fingerprint(self, request):
		        return splash_request_fingerprint(request)

2. scrapyd的使用
	1. 开启scrapyd的服务
		sudo scrapyd 或 scrapyd
	2. 把scrapy项目添加到scrapyd服务中
		修改scrapy.cfg，指定scrapyd服务的url
		在项目路径下执行 scrapyd-deploy -p 项目名
	3. 通过webapi控制scrapy爬虫
		启动：http://localhost:6800/schedule.json
			project:项目名
			spider:爬虫名
			retrun=jobid
		停止：http://localhost:6800/cancel.json
			project:项目名
			job:jobid

多爬虫管理工具框架
gerapy & spiderkeeper
https://cuiqingcai.com/4959.html
https://www.jianshu.com/p/5c76ab84e6d8

日志异常管理工具
sentry8
https://www.jianshu.com/p/d718ec9d9fbf
elk
