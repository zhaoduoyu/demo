1. 异步和非阻塞的区别
	异步是过程，非阻塞强调的是状态

2. pymongo模块
	from pymongo import MongoClient
	# client = MongoClient(host=, port=)
	uri = 'mongodb://账号:密码@127.0.0.1'
	client = MongoClient(uri, port=27017) # 连接对象
	# col = client['数据库名']['集合名']'
	col = client.数据库名.集合名

	col.insert({一条文档}/[{}, {}, ...])
	col.find_one({条件})
	rets = col.find({条件}) # 返回的是只能遍历一次的cursor游标对象
	col.delete_one({条件})
	col.delete_many({条件})
	col.update({条件},
			   {'$set':{指定更新的kv}},
			   multi=False/True, # 默认False表示只更新一条
			   upsert=False/True) # 默认False，True表示没有就插入，存在就更新

3. scrapy简单使用
	创建项目 scrapy startproject 项目名
	在项目路径下创建爬虫 scrapy genspider 爬虫名 爬取范围的域名
	在项目路径下运行爬虫 scrapy crawl 爬虫名

4. spider.py爬虫模块
	class Spider(scrapy.Spider):
		name = 爬虫名
		allowed_domains = ['爬取范围的域名', '可以是多个']
		start_urls = ['起始的url', '可以是多个']
		# scrapy.Spider类必须有名为parse的解析函数
		def parse(self, response):
			# 专门解析起始url对应的response
			yield item # {} BaseItem request None

5. scrapy提取的方法
	response.xpath(xpath_str) # 返回由selector对象构成的类list
	response.xpath(xpath_str).extract() # 返回包含字符串的列表
	response.xpath(xpath_str).extract_first() # 返回列表中第一个字符串

6. response响应对象的常用属性
	response.url
	response.request.url
	response.headers
	response.request.headers
	response.status
	response.body # 响应内容 bytes

7. pipelines.py管道需要在settings.py中开启设置

8. 在settings.py中ROBOTSTXT_OBAY改为False表示忽略robots协议

9. scrapy框架的352阵容
	三个内置数据对象
		request: url headers method post_data
		response: url headers status body
		item: {}/BaseItem
	五大模块
		scheduler调度器
		downloader下载器
		spider爬虫
		pipeline管道
		engine引擎
	两个中间件：对request、response预处理
		spider_middlewares爬虫中间件
		downloader_middlewares下载中间件

10. scrapy的工作流程
	a. spider中把start_url构造成request
	b. request--爬虫中间件--引擎--调度器，把request放入请求队列
	c. 调取器从队列中取出request--引擎--下载中间件--下载器，发送请求，获取response
	d. response--下载中间件--引擎--爬虫中间件--spider
	e. spider对response提取，提取url，构造成request--重复b步骤
	f. spider对response提取，提取item--引擎--管道