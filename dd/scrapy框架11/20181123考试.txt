import os

"""
有一个文件 UTF-8 的文本格式的文件，大小都 为 100G，计算 UTF-8 编码格式文件中的字符个数，计算机内存为 128 M
"""
# char_nums = 0
# with open('./test', 'r', encoding='utf8') as f:
#     while True:
#         ret = f.read(1024) # 这值可以再优化 psutil os sys
#         if not ret: break
#         char_nums += len(ret)
# print(char_nums)

"""
如何实现监控爬取的数据内容是否被更新
"""
# 爬取的数据在我库中，当更新写库时，触发提醒
# 爬取的数据在被爬网站中，url不变的定时轮询请求比对响应（指纹or数据），url地址变动的根据业务或内容标识更新数据（如全网抓取某品牌最新推出的手机的最低价）

"""
- 海量文本数据去重有哪些实现方案
"""
# 一致性check 指纹 or文本
# 相似性check
    # SimHash不适合做短文本去重，海明距离误判率高，只能做长文本数据之间的去重
    # 短文本抽（取标）签，使用微博领域
        #1. 剔除所有英文、数字、标点字符；
        #2. 剔除所有Twitter用户名；
        #3.分词，并标注词性，仅保留实体词性，如名词、动词；
        #4.过滤掉常用实体词（常用实体词是通过对历史训练而得，即建立自己的停止词表）；
        #5.计算保留实体词的词频，并以此为权重，选择权重大的词语作为标签；
        #6.标签数组长度大于一个阈值（如3），才认为是有信息量的锐推，否则忽略。

"""
简单描述一下 simhash 实现步骤

1、分词，把需要判断文本分词形成这个文章的特征单词。最后形成去掉噪音词的单词序列并为每个词加上权重，我们假设权重分为5个级别（1~5）。比如：“ 美国“51区”雇员称内部有9架飞碟，曾看见灰色外星人 ” ==> 分词后为 “ 美国（4） 51区（5） 雇员（3） 称（1） 内部（2） 有（1） 9架（3） 飞碟（5） 曾（1） 看见（3） 灰色（4） 外星人（5）”，括号里是代表单词在整个句子里重要程度，数字越大越重要。

2、hash，通过hash算法把每个词变成hash值，比如“美国”通过hash算法计算为 100101,“51区”通过hash算法计算为 101011。这样我们的字符串就变成了一串串数字，还记得文章开头说过的吗，要把文章变为数字计算才能提高相似度计算性能，现在是降维过程进行时。

3、加权，通过 2步骤的hash生成结果，需要按照单词的权重形成加权数字串，比如“美国”的hash值为“100101”，通过加权计算为“4 -4 -4 4 -4 4”；“51区”的hash值为“101011”，通过加权计算为 “ 5 -5 5 -5 5 5”。

4、合并，把上面各个单词算出来的序列值累加，变成只有一个序列串。比如 “美国”的 “4 -4 -4 4 -4 4”，“51区”的 “ 5 -5 5 -5 5 5”， 把每一位进行累加， “4+5 -4+-5 -4+5 4+-5 -4+5 4+5” ==》 “9 -9 1 -1 1 9”。这里作为示例只算了两个单词的，真实计算需要把所有单词的序列串累加。

5、降维，把4步算出来的 “9 -9 1 -1 1 9” 变成 0 1 串，形成我们最终的simhash签名。 如果每一位大于0 记为 1，小于0 记为 0。最后算出结果为：“1 0 1 0 1 1”。

6. 10101和00110从第一位开始依次有第一位、第四、第五位不同，则海明距离为3，海明距离越小相似度越高
"""


"""
描述一下 HMAC是什么， HMAC的客户端认证流程
"""
# HMAC是哈希运算消息认证码，由消息+密钥+某种hash算法 生成
# 服务端生成key，传给客户端；
# 客户端使用key将帐号和密码做HMAC，生成一串散列值，传给服务端；
# 服务端使用key和数据库中用户和密码做HMAC计算散列值，比对来自客户端的散列值。

"""
如何实现手机APP端爬取数据
"""
# 1. 连电脑，工具抓包拿接口 requests
# 2. appium + 抓包脚本 or 截图识别

"""
IP代理池实现思路
"""
# 收费免费多家定时抓取入库
# ip字段包括来源更新时间等
# 接口1 获取不同协议的代理ip
# 接口2 对某个代理ip 进行无效超时host标记

"""
js逆向流程
"""
# search or 右键检查 找到js代码所在位置
# 格式化并打断点查看js运行过程
# py重写 or js2py or selenium/appium/splash

"""
新闻类信息爬取下来之后如何处理
"""
# 分类 去重 分词处理 一致性 相似性 入库

"""
对于爬取的一些实时变化的网址（必须这次爬取完内容之后下一次再爬取，网址会发生变化，但是内容不会变化，这样会造成重复）是如何处理的？
"""
# 数据去重

"""
对于不同电商网站爬取的相同数据，如何区别保存的？
"""
# 复杂多字段或嵌套从一定存在一起，or 答案就在问题里需要全局唯一_id做关联
# 都能判断出来时相同数据了！！！

"""
字体反爬
"""
# 1. 字体编码变化，字体形状不变（猫眼电影）；字体编码变化，字体形状也变（汽车之家） 【两种情况】
# 2. fonttools模块 识别第二种情况有失败概率 or 截图识别，也有识别失败的概率
# 3. 人工打码 理论上没有失败概率



"""
1. 翻页请求的思路
	a. 找到下一页的url
	b. 构造请求对象，yield给引擎

2. 构造请求的方法
	scrapy.Request(url, callback)

如果判断数据内容是否更新？
1. 数据在哪
	数据库
	页面 
		一致性检查

字体反爬
	切换其他api
	截图识别
		图像的px的一致性（font code变化 px渲染不变）
		px渲染变化 
	打码平台
	font文件 fonttools
		{font_code:px渲染码}
		html_str中font_code


开话题问题：
	课堂纪要跟的慢

api函数 
	原理
	讲义上顺思路
	代码验证
	总结

课堂纪要要放在右边
pycharm背景不要，不要黑色必须白色
桌面要整理 只放授课需要的

"""